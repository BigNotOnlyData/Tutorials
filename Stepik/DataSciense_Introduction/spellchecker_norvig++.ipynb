{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Автокорректор ошибок на Python\n",
    "\n",
    "## нужно было как-то формально назвать это :)\n",
    "\n",
    "На основе блокнота *Питера Норвига, Google*\n",
    "\n",
    "Перевод и дополнения: *Ян Пиле, НИУ ВШЭ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DISCLAIMER\n",
    "Сразу предупрежу, что это мой скромный кавер на [материал](https://norvig.com/spell-correct.html) директора по исследованиям Google - Питера Норвига. Русскоязычной версии такого крутого рассказа еще не было, так что here we are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "# Импортируй и властвуй\n",
    "%pylab inline\n",
    "import re\n",
    "import math\n",
    "import string\n",
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1>Статистическая обработка естественного языка в Python.\n",
    "<br><font color=blue>или</font>\n",
    "<br>Как делать всякое со словами.  И Counter'ами (aka Счетчиками).\n",
    "<br><font color=blue>или</font>\n",
    "<br>Все, что я хотел узнать об обработке естественного языка и узнал из Улицы Сезам.\n",
    "<br>\n",
    "<br>Переводы шуток: \n",
    "<br>В русской версии улицы Сезам персонажа зовут Граф фон Знак\n",
    "<br>В английской версии это Count von Count :) \n",
    "<br>- как раз объект, который мы будем использовать\n",
    "<br><img src='http://norvig.com/ipython/the-count.jpg'> \n",
    "<br>*One, two, three, ah, ah, ah!* &mdash; The Count\n",
    "</center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Данные: Текст и Слова\n",
    "========\n",
    "\n",
    "Прежде чем что-то делать со словами(править опечатки, например), надо эти слова откуда-то взять.  Придется найти некий *текст*, возможно - из *файла*, а возможно - лежащий где-то на просторах интернета.  Затем нужно этот текст разбить на слова.  Мы позаимствуем текст у Питера Норвига - автора одного из очень популярных алгоритмов проверки правописания (и директора по исследованиям Google) :)  [вот текст](https://norvig.com/big.txt).  Считаем его из интернета и посмотрим, насколько фрагмент текста велик (в количестве символов):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6488666"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT = requests.get('https://norvig.com/big.txt').text\n",
    "len(TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст огромен - более 6 млн. символов.\n",
    "\n",
    "Теперь нужно разбить текст на слова (Ну, или если быть чуть более формальным, *токены*).  Пока что мы не будем сильно задумываться о том, что в тексте есть пунктуация, цифры и прочие символы - сосредоточимся только на словах, состоящих из букв."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(text):\n",
    "    \"\"\"Возвращает список токенов (подряд идущих буквенных последовательностей) в тексте. \n",
    "       Текст при этом приводится к нижнему регистру.\"\"\"\n",
    "    return re.findall(r'[a-z]+', text.lower()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'a', 'test', 'this', 'is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('This is: A test, 1, 2, 3, this is.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1105285"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WORDS = tokens(TEXT)\n",
    "len(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Остался миллион слов.  Вот первые 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']\n"
     ]
    }
   ],
   "source": [
    "print(WORDS[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочу заметить, что сейчас слова появляются в нашем списке в том порядке, как они располагались в тексте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Модель: Мешок слов (aka Bag of Words)\n",
    "====\n",
    "\n",
    "Мы создали список `WORDS` - список слов в том порядке, как они следуют в `TEXT`. Мы можем использовать этот список в качестве *порождающей модели* (generative model) текста. Язык - очень сложная штука и мы создаем крайне упрощенную модель языка, которая может ухватить часть этой сложной структуры.  В модели *мешка слов* , мы полностью игнорируем порядок слов, зато соблюдаем их частоту.  Представить это можно себе так: вы берете все слова текста и забрасываете их в мешок.  Теперь, если вы хотите сгенерировать предложение с помощью этого мешка, вы просто трясете его(слова там перемешиваются) и достаете указанное количество слов по одному (мешок непрозрачный, так что слоа вы достаете наугад).  Почти наверное полученное предложение будет грамматически некорректным, но слова в этом предложении будут в +- правильной пропорции (более частые будут встречаться чаще, более редкие - реже).  Вот функция, которая сэмплирует(от англ. sample) предложение из *n* слов с помощью нашего мешка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(bag, n=10):\n",
    "    \"Sample a random n-word sentence from the model described by the bag of words.\"\n",
    "    return ' '.join(random.choice(bag) for _ in range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is provision there glands which of his for her marriage'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другое представление мешка слов - `Counter`. Это словарь, состоящий из пар `{'слово': кол-во вхождений слова в текст}`.  Например,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'is': 2, 'this': 1, 'a': 2, 'test': 2, 'it': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(tokens('Is this a test? It is a test!'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Counter` очень похож на словарь из Python - тип `dict` ,  но у него есть ряд дополнительных методов.  Давайте завернем в `Counter` наш список слов `WORDS` и посмотрим, что получится:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 80030), ('of', 40025), ('and', 38313), ('to', 28766), ('in', 22050), ('a', 21155), ('that', 12512), ('he', 12401), ('was', 11410), ('it', 10681)]\n"
     ]
    }
   ],
   "source": [
    "COUNTS = Counter(WORDS)\n",
    "\n",
    "print(COUNTS.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80030 the\n",
      "83 rare\n",
      "38313 and\n",
      "0 neverbeforeseen\n",
      "460 words\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('the rare and neverbeforeseen words'):\n",
    "    print(COUNTS[w], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В 1935, лингвист Джордж Ципф отметил, что в любом большом тексте *n*-тое наиболее часто встречающееся слово появляется с частотой  ~ 1/*n* от частоты наиболее часто встречающегося слова. Это наблюдение получило название *Закона Ципфа*, несмотря на то, что Феликс Ауэрбах заметил это еще в 1913 году.  Если нарисовать частоты слов, начиная от самого часто встречающегося, на log-log-графике, они должны приблизительно следовать прямой линии, если закон Ципфа верен.  Для нашего случая все, вроде бы, +- совпадает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "M = COUNTS['the']\n",
    "yscale('log')\n",
    "xscale('log')\n",
    "title('Частота n-того наиболее частого слова и линия 1/n.')\n",
    "plot([c for (w, c) in COUNTS.most_common()], color='blue', label='real')\n",
    "plot([M/i for i in range(1, len(COUNTS)+1)], color='red', label='law Zipf')\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'abcd' -> 'abcd'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Задача: Проверка Правописания\n",
    "========\n",
    "\n",
    "Для данного слова *w* нужно найти наиболее вероятную правку *c* = `correct(`*w*`)`.\n",
    "\n",
    "**Подход:** Найти все кандидаты *c*, достаточно близкие к *w*. Выбрать наиболее вероятный из них.\n",
    "\n",
    "Осталось понять, что такое *близкие* и *наиболее вероятный*.\n",
    "\n",
    "Применим наивный подход: всегда будем брать *более близкое слово*, если проверки на близость недостаточно, берем слово с максимальной частотой из `WORDS`.  Сейчас мы будем измерять близость с помощью *расстояния Левенштейна*: минимального необходимого количества удалений, перестановок, вставок, и замен символов, необходимых чтобы одно слово превратить в другое. Конечно же это не единственный возможный подход. Методом проб и ошибок можно понять, что поиск слов в пределах расстояния 2 уже даст пристойные результаты (или можно почитать в литературе).  Тогда остается определить функцию`correct(`*w*`)`:\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(word):\n",
    "    \"Поиск лучшего исправления ошибки для данного слова.\"\n",
    "    # предрассчитать edit_distance==0, затем 1, затем 2; в противном случае оставить слово \"как есть\".\n",
    "    candidates = (known(edits0(word)) or \n",
    "                  known(edits1(word)) or \n",
    "                  known(edits2(word)) or \n",
    "                  [word])\n",
    "    return max(candidates, key=COUNTS.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции `known` и `edits0` простые; функция `edits2` iлегко получается из функции `edits1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words):\n",
    "    \"Вернуть подмножество слов, которое есть в нашем словаре.\"\n",
    "    return {w for w in words if w in COUNTS}\n",
    "\n",
    "def edits0(word): \n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 0 от word (т.е., просто само слово).\"\n",
    "    return {word}\n",
    "\n",
    "def edits2(word):\n",
    "    \"Вернуть все строки, которые находятся на edit_distance == 2 от word.\"\n",
    "    return {e2 for e1 in edits1(word) for e2 in edits1(e1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция `edits1(word)`должна возвращать множество слов, находящихся на расстоянии edit_distance == 1. Например для слова `\"wird\"` это множество будет включать слова `\"weird\"` (вставка буквы `e`) и слово `\"word\"` (замена `i` на `o`), , а также `\"iwrd\"` (перемена `w` и `i` местами); после чего может быть применена функция `known` для фильтрации и выбора подходящих кандидатов). Как же нам получить их?  Например можно *разбить* исходное слово на пару всеми возможными способами (каждое *разбиение* даст нам пару \"слов\"), `(a, b)`, первая часть - до места разбиения, а вторая - после, и в каждом месте разбиения можно: удалить, поменять местами, заменить или вставить букву:\n",
    "\n",
    "<table>\n",
    "  <tr><td> пары: <td><tt> Ø+wird <td><tt> w+ird <td><tt> wi+rd <td><tt>wir+d<td><tt>wird+Ø<td><i>Notes:</i><tt> (<i>a</i>, <i>b</i>)</tt> пара</i>\n",
    "  <tr><td> удаления: <td><tt>Ø+ird<td><tt> w+rd<td><tt> wi+d<td><tt> wir+Ø<td><td><i>Удаление первой буквы в b</i>\n",
    "  <tr><td> перемена мест: <td><tt>Ø+iwrd<td><tt> w+rid<td><tt> wi+dr</tt><td><td><td><i>Перемена мест двух первых букв b\n",
    "  <tr><td> замена: <td><tt>Ø+?ird<td><tt> w+?rd<td><tt> wi+?d<td><tt> wir+?</tt><td><td><i>замена буквы в начале b\n",
    "  <tr><td> вставка: <td><tt>Ø+?+wird<td><tt> w+?+ird<td><tt> wi+?+rd<td><tt> wir+?+d<td><tt> wird+?+Ø</tt><td><i>Вставка буквы между a и b\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"Возвращает список всех строк на расстоянии edit_distance == 1 от word.\"\n",
    "    pairs      = splits(word)\n",
    "    deletes    = [a+b[1:]           for (a, b) in pairs if b]\n",
    "    transposes = [a+b[1]+b[0]+b[2:] for (a, b) in pairs if len(b) > 1]\n",
    "    replaces   = [a+c+b[1:]         for (a, b) in pairs for c in alphabet if b]\n",
    "    inserts    = [a+c+b             for (a, b) in pairs for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def splits(word):\n",
    "    \"Возвращает список всех возможных разбиений слова на пару (a, b).\"\n",
    "    return [(word[:i], word[i:]) \n",
    "            for i in range(len(word)+1)]\n",
    "\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 'wird'), ('w', 'ird'), ('wi', 'rd'), ('wir', 'd'), ('wird', '')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits('wird')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wird'}\n"
     ]
    }
   ],
   "source": [
    "print(edits0('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wnrd', 'aird', 'lwird', 'wgird', 'word', 'zird', 'wirzd', 'ewird', 'wdird', 'wqird', 'widr', 'wiru', 'wqrd', 'awird', 'jwird', 'wikrd', 'mird', 'wirdb', 'wirnd', 'wird', 'wkrd', 'wrrd', 'wirhd', 'nird', 'wcird', 'uwird', 'wlird', 'wxird', 'wirm', 'wigrd', 'whrd', 'wirgd', 'wipd', 'wiryd', 'wirdh', 'wibd', 'wizd', 'wirb', 'wirdi', 'gwird', 'wirdd', 'dwird', 'wirdg', 'wirfd', 'wilrd', 'wzird', 'wirad', 'wiri', 'wigd', 'wicrd', 'zwird', 'wfird', 'wirg', 'wiry', 'wirdu', 'wirj', 'wbird', 'uird', 'rwird', 'wijd', 'wizrd', 'qwird', 'wirdp', 'tird', 'wtrd', 'wirdl', 'fwird', 'wirwd', 'wirds', 'wrird', 'wjrd', 'wifrd', 'wiird', 'wjird', 'wierd', 'wrd', 'wiprd', 'wiord', 'wircd', 'wied', 'wirbd', 'woird', 'wir', 'wzrd', 'wind', 'wpird', 'wirdy', 'wirjd', 'wirdf', 'wirs', 'wirw', 'wixd', 'wirdz', 'wirdq', 'wirud', 'kwird', 'wtird', 'wkird', 'bird', 'wiurd', 'wuird', 'wirc', 'wid', 'wirvd', 'wirp', 'wlrd', 'wixrd', 'wiqrd', 'wirdt', 'wirz', 'kird', 'owird', 'wirdj', 'iwird', 'ward', 'xird', 'wivd', 'mwird', 'waird', 'wirdv', 'xwird', 'wiyrd', 'wirod', 'wbrd', 'swird', 'wirr', 'wwird', 'iird', 'wmird', 'cird', 'wirn', 'pwird', 'wirdo', 'oird', 'sird', 'wurd', 'witd', 'wiyd', 'wmrd', 'wirq', 'hwird', 'nwird', 'whird', 'weird', 'vird', 'wiwrd', 'wirf', 'yird', 'wrid', 'wirx', 'pird', 'wikd', 'wirrd', 'wisrd', 'wirdc', 'wirdr', 'qird', 'werd', 'wifd', 'lird', 'wirxd', 'jird', 'gird', 'vwird', 'wirt', 'wibrd', 'hird', 'wgrd', 'wiard', 'wirsd', 'wirh', 'wisd', 'rird', 'wirde', 'wiod', 'wirdm', 'wiid', 'bwird', 'wwrd', 'cwird', 'wprd', 'iwrd', 'wyrd', 'wsird', 'wirdk', 'wirdw', 'twird', 'fird', 'wiud', 'wire', 'wijrd', 'eird', 'wihd', 'wfrd', 'dird', 'wiqd', 'wirkd', 'wira', 'wsrd', 'wirqd', 'witrd', 'wvird', 'wivrd', 'wirda', 'wvrd', 'wirv', 'wiro', 'wirk', 'wirl', 'wirmd', 'wicd', 'wimd', 'ird', 'wirpd', 'wild', 'wdrd', 'winrd', 'wirid', 'wcrd', 'wirld', 'wirtd', 'widrd', 'wimrd', 'wyird', 'wirdn', 'wired', 'wnird', 'wiad', 'wihrd', 'wxrd', 'wirdx', 'widd', 'wiwd', 'ywird'}\n"
     ]
    }
   ],
   "source": [
    "print(edits1('wird'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24254\n"
     ]
    }
   ],
   "source": [
    "print(len(edits2('wird')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speling',\n",
       " 'errurs',\n",
       " 'in',\n",
       " 'somethink',\n",
       " 'whutever',\n",
       " 'unusuel',\n",
       " 'misteakes',\n",
       " 'everyware']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens('Speling errurs in somethink. Whutever; unusuel misteakes everyware?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spelling',\n",
       " 'errors',\n",
       " 'in',\n",
       " 'something',\n",
       " 'whatever',\n",
       " 'unusual',\n",
       " 'mistakes',\n",
       " 'everywhere']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(correct, tokens('Speling errurs in somethink. Whutever; unusuel misteakes everyware?')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно ли сделать выходные данные покрасивее?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(text):\n",
    "    \"Исправить все слова с опечатками в тексте.\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)\n",
    "\n",
    "def correct_match(match):\n",
    "    \"Исправить слово word в match-группе, сохранив регистр: upper/lower/title.\"\n",
    "    word = match.group()\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def case_of(text):\n",
    "    \"Возвращает функцию регистра по тексту: upper, lower, title, или str.\"\n",
    "    return (str.upper if text.isupper() else\n",
    "            str.lower if text.islower() else\n",
    "            str.title if text.istitle() else\n",
    "            str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fhgwdjksa'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str('fhgwdjksa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<method 'upper' of 'str' objects>,\n",
       " <method 'lower' of 'str' objects>,\n",
       " <method 'title' of 'str' objects>,\n",
       " str]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(case_of, ['UPPER', 'lower', 'Title', 'CamelCase']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spelling Errors IN something. Whatever; unusual mistakes?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Speling Errurs IN somethink. Whutever; unusuel misteakes?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Audience says: tumbler ...'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text('Audiance sayzs: tumblr ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уже неплохо. Возможно вы уже придумали десяток способов сделать все это получше.  Я предложу один: например в тексте \"three, too, one, blastoff!\" вместо \"too\" явно нужно подставить \"two\", несмотря на то что слово \"too\" имеется в нашем словаре.  Мы могли бы улучшить результат, приняв во внимание *последовательность* слов, а не только отдельные слова.  Ну и как же мы будем выбирать скорректированные слова тогда, спросите вы?   Ad-hoc подход неплохо сработал для отдельных слов, давайте поддадим немного теории."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Теория: От счетчика слов к вероятностям последовательностей слов\n",
    "===\n",
    "\n",
    "Нам нужно научиться подсчитывать вероятности слов, $P(w)$.  Делать мы это будем с помощью функции `pdist`, которая на вход принимает `Counter` (наш мешок слов) и возвращает функцию, выполняющую роль вероятностного распределения на множестве всех возможных слов.  В вероятностном распределении вероятность каждого слова лежит между 0 и 1, и сложение вероятностей всех слов дает 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist(counter):\n",
    "    \"Превращает частоты из Counter в вероятностное распределение.\"\n",
    "    N = sum(list(counter.values()))\n",
    "    return lambda x: counter[x]/N\n",
    "\n",
    "P = pdist(COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07240666434449033 the\n",
      "0.008842968103249388 is\n",
      "0.07240666434449033 the\n",
      "0.0008215075749693518 most\n",
      "0.0002596615352601365 common\n",
      "0.0002696137195383996 word\n",
      "0.019949605757790978 in\n",
      "0.00019090098933759167 english\n"
     ]
    }
   ],
   "source": [
    "for w in tokens('\"The\" is the most common word in English'):\n",
    "    print(P(w), w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, что же такое вероятность *последовательности* слов?  Используем определение совместной вероятности:\n",
    "\n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_1 w_2) \\ldots  \\times \\ldots P(w_n \\mid w_1 \\ldots w_{n-1})$\n",
    "\n",
    "Модель *мешка слов* подразумевает, что каждое слово из мешка достается *независимо* от других.  Это дает нам ~~неправильную~~ упрощенную аппроксимацию:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2) \\times P(w_3) \\ldots  \\times \\ldots P(w_n)$\n",
    "\n",
    "    \n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/a/a2/GeorgeEPBox.jpg/200px-GeorgeEPBox.jpg\"> Известный статистик Джордж Бокс сказал *Все модели неверны, но некоторые полезны.*\n",
    "    \n",
    "Как же нам посчитать $P(w_1 \\ldots w_n)$?  Мы будем использовать другое название, чтобы не обманывать себя, `Pwords` вместо `P`, и посчитаем ее как произведение индивидуальных вероятностей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords(words):\n",
    "    \"Вероятности слов, при условии, что они независимы.\"\n",
    "    return product(P(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Перемножим числа.  (Это как `sum`, только с умножением.)\"\n",
    "    result = 1\n",
    "    for x in nums:\n",
    "        result *= x\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.983396332800731e-11 this is a test\n",
      "8.637472023018802e-16 this is a unusual test\n",
      "0.0 this is a neverbeforeseen test\n"
     ]
    }
   ],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a neverbeforeseen test']\n",
    "\n",
    "for test in tests:\n",
    "    print(Pwords(tokens(test)), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВОУ&mdash;кажется, присвоить последнюю вероятность 0, неправильно; Она просто должна быть маленькой.  К этому вернемся попозже.  Ну а другие вероятности кажутся +- адекватными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Задача: Разбиение слов на сегменты\n",
    "====\n",
    "\n",
    "**Задача**: *Разбить полученную последовательность символов без пробелов на последовательность слов\n",
    "\n",
    "Зачем? Есть языки, в которых пробелов нет: [不带空格的词](http://translate.google.com/#auto/en/%E4%B8%8D%E5%B8%A6%E7%A9%BA%E6%A0%BC%E7%9A%84%E8%AF%8D)\n",
    "\n",
    "В английском тоже есть ситуации, когда слова пишутся слитно (по ошибке или нет) ([spelling errors](https://www.google.com/search?q=wordstogether), [URLs](http://speedofart.com)).\n",
    "\n",
    "**Подход 1:** Перенумеруем все возможные разбиения и выберем то, у которого максимальная Pwords\n",
    "\n",
    "Вопрос: Как выбрать количество сегментов для строки длины *n*?\n",
    "\n",
    "**Подход 2:** Делаем одно разбиение - на первое слово и все остальное.  Если предположить, что слова независимы,\n",
    "можно максимизировать вероятность первого слова + лучшего разбиения оставшихся букв.\n",
    "    \n",
    "    assert segment('choosespain') == ['choose', 'spain']\n",
    "\n",
    "    segment('choosespain') ==\n",
    "       max(Pwords(['c'] + segment('hoosespain')),\n",
    "           Pwords(['ch'] + segment('oosespain')),\n",
    "           Pwords(['cho'] + segment('osespain')),\n",
    "           Pwords(['choo'] + segment('sespain')),\n",
    "           ...\n",
    "           Pwords(['choosespain'] + segment('')))\n",
    "       \n",
    "    \n",
    "       \n",
    "Чтобы сделать это хоть сколько-нибудь эффективным, нужно избежать слишком большого числа пересчетов оставшейся части слова.  Это можно сделать или с помощью [динамического программирования](https://habr.com/ru/post/113108/) или с помощью [мемоизации](https://ru.wikipedia.org/wiki/Мемоизация) aka кэширования. Кроме того, для первого слова не обязательно брать все возможные варианты разбиений - мы можем установить максимальную длину.  Какой она должна быть?  Чуть большей, чем длина самого длинного слова, которое мы видели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(f):\n",
    "    \"Запомнить результаты исполнения функции f, чьи аргументы args должны быть хешируемыми.\"\n",
    "    cache = {}\n",
    "    def fmemo(*args):\n",
    "        if args not in cache:\n",
    "            cache[args] = f(*args)\n",
    "        return cache[args]\n",
    "    fmemo.cache = cache\n",
    "    return fmemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переиспользование предыдущих результатов - очень интересная штука, рекомендую заглянуть [сюда](https://habr.com/ru/post/335866/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(w) for w in COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splits(text, start=0, L=20):\n",
    "    \"Вернуть список всех пар (a, b); start <= len(a) <= L.\"\n",
    "    return [(text[:i], text[i:]) \n",
    "            for i in range(start, min(len(text), L)+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', 'word'), ('w', 'ord'), ('wo', 'rd'), ('wor', 'd'), ('word', '')]\n",
      "[('r', 'eallylongtext'), ('re', 'allylongtext'), ('rea', 'llylongtext'), ('real', 'lylongtext')]\n"
     ]
    }
   ],
   "source": [
    "print(splits('word'))\n",
    "print(splits('reallylongtext', 1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def segment(text):\n",
    "    \"Вернуть список слов, который является наиболее вероятной сегментацией нашего текста.\"\n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment(rest) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иллюстрацию того, что мемоизация - суперполезная штука, можно произвести на примере расчета чисел Фибоначчи. Если вы программист, они вам, наверняка, уже оскомину набили, но мы все равно на них посмотрим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fib(40) = 102334155\n",
      "0.00024049799992553744\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "@memo\n",
    "def fib(n):\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fib(n-2) + fib(n-1)\n",
    "\n",
    "# Какое число мы хотим посчитать\n",
    "x = 40\n",
    "\n",
    "t1 = time.perf_counter()\n",
    "print(f'fib({x}) =', fib(x))\n",
    "print(time.perf_counter() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['choose', 'spain']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('choosespain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speed', 'of', 'art']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('speedofart')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "decl = ('wheninthecourseofhumaneventsitbecomesnecessaryforpeople' +\n",
    "        'todissolvethepoliticalbandswhichhaveconnectedthemwithoneanother' +\n",
    "        'andtoassumeamongthepowersoftheearththeseparateandequalstation' +\n",
    "        'towhichthelawsofnatureandofnaturesgodentitlethem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when', 'in', 'the', 'course', 'of', 'human', 'events', 'it', 'becomes', 'necessary', 'for', 'people', 'to', 'dissolve', 'the', 'political', 'bands', 'which', 'have', 'connected', 'them', 'with', 'one', 'another', 'and', 'to', 'assume', 'among', 'the', 'powers', 'of', 'the', 'earth', 'the', 'separate', 'and', 'equal', 'station', 'to', 'which', 'the', 'laws', 'of', 'nature', 'and', 'of', 'natures', 'god', 'entitle', 'them']\n"
     ]
    }
   ],
   "source": [
    "print(segment(decl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6043381425711275e-141"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2991253445993085e-281"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pwords(segment(decl * 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возникла проблема переполнения разрядности числа.  Но вернемся мы к ней чуть попозже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'and', 'insignificant']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('smallandinsignificant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['large', 'and', 'insignificant']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('largeandinsignificant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.111418791681202e-10\n",
      "1.0662753919897733e-11\n"
     ]
    }
   ],
   "source": [
    "print(Pwords(['large', 'and', 'insignificant']))\n",
    "print(Pwords(['large', 'and', 'in', 'significant']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого:\n",
    "    \n",
    "- Выглядит недурно!\n",
    "- Предположение о мешке слов имеет ряд ограничений.\n",
    "- Пересчет Pwords на каждом вызове выглядит неэффективным.\n",
    "- Переполнение чисел возникает для текстов длинее +- 100 слов; придется использовать логарифмы или еще какие-то хитрости.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Теория и практика: Еще немного о динамическом программировании.\n",
    "## Насколько дорого превращать одно слово в другое?\n",
    "\n",
    "<a name='4-1'></a>\n",
    "\n",
    "Динамическое программирование позволяет разбить задачу на подзадачи, решив которые можно скомпоновать финальное решение. Мы будем пытаться превратить строку source[0..i] в строку target[0..j], мы сосчитаем все возможные комбинации подстрок substrings[i, j] и рассчитаем их edit_distance до нашей исходной. Мы будем сохранять результаты в таблицу и переиспользовать их для расчета дальнейших изменений.\n",
    "\n",
    "Необходимо создать матрицу такого вида:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Инициализация}$$\n",
    "\n",
    "\\begin{align}\n",
    "D[0,0] &= 0 \\\\\n",
    "D[i,0] &= D[i-1,0] + del\\_cost(source[i]) \\tag{4}\\\\\n",
    "D[0,j] &= D[0,j-1] + ins\\_cost(target[j]) \\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Операции в каждой ячейке}$$\n",
    "\\begin{align}\n",
    " \\\\\n",
    "D[i,j] =min\n",
    "\\begin{cases}\n",
    "D[i-1,j] + del\\_cost\\\\\n",
    "D[i,j-1] + ins\\_cost\\\\\n",
    "D[i-1,j-1] + \\left\\{\\begin{matrix}\n",
    "rep\\_cost; & if src[i]\\neq tar[j]\\\\\n",
    "0 ; & if src[i]=tar[j]\n",
    "\\end{matrix}\\right.\n",
    "\\end{cases}\n",
    "\\tag{5}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, превратить слово **play** в слово **stay** при стоимости вставки 1, стоимости удаления 1, и стоимости замены 2 даст такую таблицу:\n",
    "<table style=\"width:20%\">\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> </b>  </td>\n",
    "    <td> <b># </b>  </td>\n",
    "    <td> <b>s </b>  </td>\n",
    "    <td> <b>t </b> </td> \n",
    "    <td> <b>a </b> </td> \n",
    "    <td> <b>y </b> </td> \n",
    "  </tr>\n",
    "   <tr>\n",
    "    <td> <b>  #  </b></td>\n",
    "    <td> 0</td> \n",
    "    <td> 1</td> \n",
    "    <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    " \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> <b>  p  </b></td>\n",
    "    <td> 1</td> \n",
    " <td> 2</td> \n",
    "    <td> 3</td> \n",
    "    <td> 4</td> \n",
    "   <td> 5</td>\n",
    "  </tr>\n",
    "   \n",
    "  <tr>\n",
    "    <td> <b> l </b></td>\n",
    "    <td>2</td> \n",
    "    <td>3</td> \n",
    "    <td>4</td> \n",
    "    <td>5</td> \n",
    "    <td>6</td>\n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> <b> a </b></td>\n",
    "    <td>3</td> \n",
    "     <td>4</td> \n",
    "     <td>5</td> \n",
    "     <td>4</td>\n",
    "     <td>5</td> \n",
    "  </tr>\n",
    "  \n",
    "   <tr>\n",
    "    <td> <b> y </b></td>\n",
    "    <td>4</td> \n",
    "      <td>5</td> \n",
    "     <td>6</td> \n",
    "     <td>5</td>\n",
    "     <td>4</td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "</table>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: строка-исходник\n",
    "        target: строка, в которую мы должны исходник превратить\n",
    "        ins_cost: цена вставки\n",
    "        del_cost: цена удаления\n",
    "        rep_cost: цена замены буквы\n",
    "    Output:\n",
    "        D: матрица размера len(source)+1 на len(target)+1 содержащая минимальные расстояния edit_distance\n",
    "        med: минимальное расстояние edit_distance (med), необходимое, \n",
    "        чтобы превратить строку source в строку target\n",
    "    '''\n",
    "    # стоимость удаления и вставки = 1\n",
    "    m = len(source)\n",
    "    n = len(target)\n",
    "\n",
    "    # Заткнем нашу матрицу нулями\n",
    "    D = np.zeros((m+1, n+1), dtype=int) \n",
    "    \n",
    "    # Заполним первую колонку\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + del_cost\n",
    "        \n",
    "    # Заполним первую строку\n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + ins_cost\n",
    "        \n",
    "    # Теперь пойдем от 1 к m-той строке\n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        # итерируемся по колонкам от 1 до n\n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            # r_cost - стоимость замены\n",
    "            r_cost = rep_cost\n",
    "            \n",
    "            # Совпадает ли буква исходного слова из предыдущей строки\n",
    "            # с буквой целевого слова из предыдущей колонки, \n",
    "            if source[row-1] == target[col-1]:\n",
    "                # Если они не нужны, то замена не нужна -> стоимость = 0\n",
    "                r_cost = 0\n",
    "                \n",
    "            # Обновляем значение ячейки на базе предыдущих значений \n",
    "            # Считаем D[i,j] как минимум из трех возможных стоимостей (как в формуле выше)\n",
    "            D[row,col] = min([D[row-1,col]+del_cost, D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost])\n",
    "          \n",
    "    # установить edit_distance в значение из правого нижнего угла\n",
    "    med = D[m,n]\n",
    "    \n",
    "\n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Расстояние:  4 \n",
      "\n",
      "   #  s  t  a  y\n",
      "#  0  1  2  3  4\n",
      "p  1  2  3  4  5\n",
      "l  2  3  4  5  6\n",
      "a  3  4  5  4  5\n",
      "y  4  5  6  5  4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "source =  'play'\n",
    "target = 'stay'\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "\n",
    "print(\"Расстояние: \",min_edits, \"\\n\")\n",
    "\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (7) Данные: Mo' Data, Mo' Better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам мало миллионов слов в \"обучающей выборке\" давайте перейдем к *МИЛЛИАРДАМ* слов.  Получив такой огромный объем информации, можно перейти к анализу пар последоваительных слов, не ожидая, что вероятности слишком часто будут обнуляться (представьте себе, сколько в языке может быть грамматически корректных сочетаний из двух слов).  Мы вновь позаимствуем уже собранные данные у мистера Норвига. Лежат они на его сайте в формате `\"word \\t count\"` для отдельных слов и в формате `\"word1 word2 \\t count\"` для биграмм.  Считаем их и упакуем в наши словари с вероятностями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts(text, sep='\\t'):\n",
    "    \"\"\"Возвращает Counter, полученный из пар ключ-значение,в каждой строке файла.\"\"\"\n",
    "    C = Counter()\n",
    "    for i in [l.split('\\t') for l in text.split('\\n')][:-1]:\n",
    "        key, count = i\n",
    "        C[key] = int(count)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для проверки кусков кода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTS1 = load_counts(requests.get('https://www.norvig.com/ngrams/count_1w.txt').text)\n",
    "COUNTS2 = load_counts(requests.get('https://www.norvig.com/ngrams/count_2w.txt').text)\n",
    "\n",
    "P1w = pdist(COUNTS1)\n",
    "P2w = pdist(COUNTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "333333 588.124220187\n",
      "286358 225.955251755\n"
     ]
    }
   ],
   "source": [
    "print(len(COUNTS1), sum(list(COUNTS1.values()))/1e9)\n",
    "print(len(COUNTS2), sum(list(COUNTS2.values()))/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ужас! Сотни миллиардов. Но мы справились."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of the', 2766332391),\n",
       " ('in the', 1628795324),\n",
       " ('to the', 1139248999),\n",
       " ('on the', 800328815),\n",
       " ('for the', 692874802),\n",
       " ('and the', 629726893),\n",
       " ('to be', 505148997),\n",
       " ('is a', 476718990),\n",
       " ('with the', 461331348),\n",
       " ('from the', 428303219),\n",
       " ('by the', 417106045),\n",
       " ('at the', 416201497),\n",
       " ('of a', 387060526),\n",
       " ('in a', 364730082),\n",
       " ('will be', 356175009),\n",
       " ('that the', 333393891),\n",
       " ('do not', 326267941),\n",
       " ('is the', 306482559),\n",
       " ('to a', 279146624),\n",
       " ('is not', 276753375),\n",
       " ('for a', 274112498),\n",
       " ('with a', 271525283),\n",
       " ('as a', 270401798),\n",
       " ('<S> and', 261891475),\n",
       " ('of this', 258707741),\n",
       " ('<S> the', 258483382),\n",
       " ('it is', 245002494),\n",
       " ('can be', 230215143),\n",
       " ('If you', 210252670),\n",
       " ('has been', 196769958)]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTS2.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(8) Теория и Практика: Сегментация с помощью биграмм\n",
    "===\n",
    "\n",
    "Чуть менее неправильная аппроксимация:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "Эта штука называется *биграммной* моделью. Представьте, что вы взяли текст, достали из него все возможные пары подряд идущих слов и положили каждую пару в мешок, промаркированный ПЕРВЫМ словом из пары.  После этого, чтобы сгенерировать кусок текста, мы берем первое слово из исходного мешка слов , а каждое следующее слово вынимаем из соответствующего мешка биграмм. \n",
    "\n",
    "Начнем с определения вероятности текущего слова при условии данного предыдущего слова из Counter:\n",
    "\n",
    "Отмечу, что для английского языка биграммная модель будет выглядеть так:\n",
    "    \n",
    "$P(w_1 \\ldots w_n) = P(w_1) \\times P(w_2 \\mid w_1) \\times P(w_3 \\mid w_2) \\ldots  \\times \\ldots P(w_n \\mid w_{n-1})$\n",
    "\n",
    "условная вероятность слова при условии предыдущего слова определяется так:\n",
    "\n",
    "$P(w_n \\mid w_{n-1}) = P(w_{n-1}w_n) / P(w_{n-1}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pwords2(words, prev='<S>'):\n",
    "    \"Вероятность последовательности слов с помощью биграммной модели(при условии предыдущего слова).\"\n",
    "    return product(cPword(w, (prev if (i == 0) else words[i-1]) )\n",
    "                   for (i, w) in enumerate(words))\n",
    "\n",
    "# Перепишем Pwords на большой словарь P1w вместо Pword\n",
    "def Pwords(words):\n",
    "    \"Вероятности слов при условии их независимости.\"\n",
    "    return product(P1w(w) for w in words)\n",
    "\n",
    "def cPword(word, prev):\n",
    "    \"Условная вероятность слова при условии предыдущего.\"\n",
    "    bigram = prev + ' ' + word\n",
    "    if P2w(bigram) > 0 and P1w(prev) > 0:\n",
    "        return P2w(bigram) / P1w(prev)\n",
    "    else: # если что-то не встретилось, поставим среднее между P1w и 0\n",
    "        return P1w(word) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7873982000630825e-10\n",
      "6.413676294377262e-08\n",
      "1.1802860036709024e-11\n"
     ]
    }
   ],
   "source": [
    "print(Pwords(tokens('this is a test')))\n",
    "print(Pwords2(tokens('this is a test')))\n",
    "print(Pwords2(tokens('is test a this')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать `segment2`, скопируем `segment`, добавим в аргументы предыдущий токен, а вероятности будем считать с помощью `Pwords2` вместо `Pwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo \n",
    "def segment2(text, prev='<S>'): \n",
    "    \"Возвращает наилучшее разбиение текста, используя статистику биграмм.\" \n",
    "    if not text: \n",
    "        return []\n",
    "    else:\n",
    "        candidates = ([first] + segment2(rest, first) \n",
    "                      for (first, rest) in splits(text, 1))\n",
    "        return max(candidates, key=lambda words: Pwords2(words, prev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['choose', 'spain']\n",
      "['speed', 'of', 'art']\n",
      "['small', 'and', 'in', 'significant']\n",
      "['large', 'and', 'in', 'significant']\n"
     ]
    }
   ],
   "source": [
    "print(segment2('choosespain'))\n",
    "print(segment2('speedofart'))\n",
    "print(segment2('smallandinsignificant'))\n",
    "print(segment2('largeandinsignificant'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кусочек из \"Автостопом по галактике\" Дугласа Адамса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['far', 'out', 'in', 'the', 'uncharted', 'backwaters', 'of', 'the', 'unfashionable', 'end', 'of', 'the', 'western', 'spiral', 'arm', 'of', 'the', 'galaxy', 'lies', 'a', 'small', 'un', 'regarded', 'yellow', 'sun']\n",
      "['far', 'out', 'in', 'the', 'uncharted', 'backwaters', 'of', 'the', 'unfashionable', 'end', 'of', 'the', 'western', 'spiral', 'arm', 'of', 'the', 'galaxy', 'lies', 'a', 'small', 'un', 'regarded', 'yellow', 'sun']\n"
     ]
    }
   ],
   "source": [
    "adams = ('faroutintheunchartedbackwatersoftheunfashionableendofthewesternspiral' +\n",
    "         'armofthegalaxyliesasmallunregardedyellowsun')\n",
    "print(segment(adams))\n",
    "print(segment2(adams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1w('unregarded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'herecomeoldflattophecomegroovingupslowlyhegotjoojooeyeballheoneholyrollerhegothairdowntohiskneegottobeajokerhejustdowhatheplease'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beatles = \"\"\"Here come old flattop he come grooving up slowly \n",
    "          He got joo-joo eyeball he one holy roller \n",
    "          He got hair down to his knee \n",
    "          Got to be a joker he just do what he please\"\"\"\n",
    "beatles = ''.join(re.findall(r'[a-z]',beatles.lower()))\n",
    "beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'dry', 'bare', 'sandy', 'hole', 'with', 'nothing', 'in', 'it', 'to', 'sitdown', 'on', 'or', 'to', 'eat']\n",
      "['a', 'dry', 'bare', 'sandy', 'hole', 'with', 'nothing', 'in', 'it', 'to', 'sit', 'down', 'on', 'or', 'to', 'eat']\n"
     ]
    }
   ],
   "source": [
    "tolkien = 'adrybaresandyholewithnothinginittositdownonortoeat'\n",
    "print(segment(tolkien))\n",
    "print(segment2(tolkien))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['here', 'come', 'old', 'flattop', 'he', 'come', 'grooving', 'up', 'slowly', 'he', 'got', 'joo', 'joo', 'eyeball', 'he', 'one', 'holy', 'roller', 'he', 'got', 'hair', 'down', 'to', 'his', 'knee', 'got', 'to', 'be', 'a', 'joker', 'he', 'just', 'do', 'what', 'he', 'please']\n",
      "['here', 'come', 'old', 'flattop', 'he', 'come', 'grooving', 'up', 'slowly', 'he', 'got', 'joo', 'joo', 'eyeball', 'he', 'one', 'holy', 'roller', 'he', 'got', 'hair', 'down', 'to', 'his', 'knee', 'got', 'to', 'be', 'a', 'joker', 'he', 'just', 'do', 'what', 'he', 'please']\n"
     ]
    }
   ],
   "source": [
    "print(segment(beatles))\n",
    "print(segment2(beatles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и что теперь? Биграммная модель вроде бы лучше, но не очень.  Сотен миллиардов слов все равно может быть недостаточно. (Ну а почему бы не триллион слов?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(9) Теория: Валидация\n",
    "===\n",
    "До настоящего момента мы пытались интуитивно оценить результаты нашей работы. Тем не менее, никаких численных оценок качества мы пока не получили.  Важно понимать, что без четких метрик слова \"плохо\"/\"хорошо\" не имеют никакого смысла. Более того - мы даже не можем четко ответить, было ли наше обновление модели в лучшую сторону или худшую.\n",
    "Обычно при построении неких прогностических моделей данные разбиваются на три части:\n",
    "<ol>\n",
    "  <li> <b>Обучающая выборка:</b> То, что мы использовали для создания модели исправления ошибок; У нас это был файл <tt>big.txt</tt> file.\n",
    "  <li> <b>Тестовая выборка:</b> Набор данных, который можно использовать для оценки качества вашей модели по ходу разработки.\n",
    "  <li> <b>Валидационная выборка:</b> Набор данных, который мы используем для оценки работы программы <i>после того</i> как программа готова.  Тестовая выборка для этого быть использована не может&mdash;Стоит\n",
    "   разработчику посмотреть на результаты на тестовой выборке, она уже \"испорчена\". В принципе, программист может изменить программу так, чтобы она \"подгонялась\" под тестовую выборку, а это будет \"переобучением\". Вот почему нам нужен отдельный набор тестов, который рассматривается только после завершения разработки..\n",
    "</ol>\n",
    "\n",
    "Для нашей программы обучающая выборка - словарь с частотами слов, а тестовая выборка -  набор примеров типа `\"choosespain\"`, на которых мы отлаживались. Остается сделать валидационную выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_segmenter(segmenter, tests):\n",
    "    \"Оценка сегментатора на тестовых данных; вывести на печать ошибки; вернуть долю верно разбитого.\"\n",
    "    return sum([test_one_segment(segmenter, test) \n",
    "               for test in tests]), len(tests)\n",
    "\n",
    "def test_one_segment(segmenter, test):\n",
    "    words = tokens(test)\n",
    "    result = segmenter(''.join(words))\n",
    "    correct = (result == words)\n",
    "    if not correct:\n",
    "        print('expected', words)\n",
    "        print('got     ', result) \n",
    "    return correct\n",
    "\n",
    "proverbs = (\"\"\"A little knowledge is a dangerous thing\n",
    "  A man who is his own lawyer has a fool for his client\n",
    "  All work and no play makes Jack a dull boy\n",
    "  Better to remain silent and be thought a fool that to speak and remove all doubt;\n",
    "  Do unto others as you would have them do to you\n",
    "  Early to bed and early to rise, makes a man healthy, wealthy and wise\n",
    "  Fools rush in where angels fear to tread\n",
    "  Genius is one percent inspiration, ninety-nine percent perspiration\n",
    "  If you lie down with dogs, you will get up with fleas\n",
    "  Lightning never strikes twice in the same place\n",
    "  Power corrupts; absolute power corrupts absolutely\n",
    "  Here today, gone tomorrow\n",
    "  See no evil, hear no evil, speak no evil\n",
    "  Sticks and stones may break my bones, but words will never hurt me\n",
    "  Take care of the pence and the pounds will take care of themselves\n",
    "  Take care of the sense and the sounds will take care of themselves\n",
    "  The bigger they are, the harder they fall\n",
    "  The grass is always greener on the other side of the fence\n",
    "  The more things change, the more they stay the same\n",
    "  Those who do not learn from history are doomed to repeat it\"\"\"\n",
    "  .splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected ['sticks', 'and', 'stones', 'may', 'break', 'my', 'bones', 'but', 'words', 'will', 'never', 'hurt', 'me']\n",
      "got      ['stick', 'sandstones', 'may', 'break', 'my', 'bones', 'but', 'words', 'will', 'never', 'hurt', 'me']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19, 20)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment, proverbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_segmenter(segment2, proverbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что ж, оба наших сегментатора неплохи, а `segment2` показал себя лучше. Рекомендую сочинить еще тестов и подумать, как можно было бы оценить статистическую значимость."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(10) Теория и Практика: Сглаживание\n",
    "======\n",
    "\n",
    "Вернемся к нашему предыдущему тесту и добавим еще несколько случаев для проверки:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78739820006e-10 this is a test\n",
      "3.78675425278e-15 this is a unusual test\n",
      "1.31179474235e-16 this is a nongovernmental test\n",
      "0.0 this is a neverbeforeseen test\n",
      "0.0 this is a zqbhjhsyefvvjqc test\n"
     ]
    }
   ],
   "source": [
    "tests = ['this is a test', \n",
    "         'this is a unusual test',\n",
    "         'this is a nongovernmental test',\n",
    "         'this is a neverbeforeseen test',\n",
    "         'this is a zqbhjhsyefvvjqc test']\n",
    "\n",
    "for test in tests:\n",
    "    print Pwords(tokens(test)), test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблема в том, что вероятность одного из слов - 0.  Среди этих трех 15-букваенных слов, \"nongovernmental\" в нашем словаре есть, но если б его не было, вся вероятность бы обнулилась (мы же считаем произведение).  Кажется, что это слишком строгое условие; Словарь не идеален и точно существуют реальные слова, которых мы не увидели. Давайте не будем все сразу обнулять.  Например точно должна быть оценка вероятности того, что слово настоящее.  Скажем,  \"neverbeforeseen\" уж явно более английское чем \"zqbhjhsyefvvjqc\" и должно иметь бОльшую вероятность.\n",
    "\n",
    "Проблему можно побороть, присвоив таким \"не встретившимся\" словам ненулевую вероятность.  Еще более важным этот пункт становится при переходе к токенам из нескольких слов (биграммам, например), потому что чем больше слов в токене, тем больше вероятность, что какой-то реальный токен в нашей обучающей выборке отсутствует.\n",
    "\n",
    "Нашу модель можно представить в виде забора вероятностей, где столбик равен вероятности слова/токена, которое/который в выборке было/был, и равен 0, если слова/токена в выборке не было; Мы хотим *сгладить* наше распределение вокруг этих пиков, чтобы модель давала какой-то ответ вне зависимости от наличия или отсутствия слова в корпусе. Этот процесс и называется *сглаживанием*.\n",
    "\n",
    "**Место для анекдота про Лапласа**\n",
    "\n",
    "Однажды французского математика Лапласа спросили: \"Какова вероятность того, что Солнце завтра взойдет?\".  Из данных, что оно из $n$ ближайших дней взошло $n$ раз следует оценка максимального правдоподобия $n/n$ = 1.  Но Лапласу хотелось чуть сбалансировать оценку на шанс того, что завтра Солнце может и не взойти, поэтому он дал оценку $(n + 1) / (n + 2)$.\n",
    "\n",
    "Подробнее о задачке про Солнце [тут](https://en.wikipedia.org/wiki/Sunrise_problem)\n",
    "\n",
    "\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Pierre-Simon_Laplace.jpg/220px-Pierre-Simon_Laplace.jpg\" height=150 width=110> \n",
    "&nbsp;\n",
    "<img src=\"http://www.hdwallpapers.in/walls/notre_dame_at_sunrise_paris_france-normal.jpg\" width=200 height=150>\n",
    "<br><i>То, что мы знаем, ограничено, а то, чего мы не знаем,-бесконечно<i><br>&mdash; Пьер Симон Лаплас, 1749-1827"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_additive_smoothed(counter, c=1):\n",
    "    \"\"\"Вероятность слова, при условии данных из Counter'a.\n",
    "    добавляем c к частоте каждого слова + слово 'unknown'.\"\"\"\n",
    "    N = sum(list(counter.values()))          # суммарное кол-во слов\n",
    "    Nplus = N + c * (len(counter) + 1) # кол-во слов + сглаживание\n",
    "    return lambda word: (counter[word] + c) / Nplus \n",
    "\n",
    "P1w = pdist_additive_smoothed(COUNTS1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формулу Лапласовского сглаживания посмотреть можно [тут](https://en.wikipedia.org/wiki/Additive_smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7003201005861308e-12"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P1w('neverbeforeseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь еще одна проблема ... у нас появились незнакомые слова с ненулевой вероятностью.  А что если 10<sup>-12</sup> - приемлемая вероятность для слов нашего текста: то есть, если я *читаю* новый текст, вероятность того, что следующее слово мне незнакомо, может быть порядка 10<sup>-12</sup>.  Но если я случайно *генерирую* 20-буквенный последовательности, вероятность того, что одна из них будет реальным словом намного меньше чем 10<sup>-12</sup>.  \n",
    "\n",
    "Смотрите, что происходит:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'of',\n",
       " 'along',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment('thisisatestofsegmentationofalongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас две проблемы:\n",
    "    \n",
    "Во-первых, у нас нет четкой модели для неизвестных слов.  Мы говорим \"неизвестное слово\", но не различаем более\n",
    "вероятные неизвестные слова и менее вероятные неизвестные слова.  Ну, например, вероятнее ли 8-буквенное неизвестное слово чем 20-буквенное неизвестное слово?\n",
    "\n",
    "Во-вторых, мы не берем в расчет информацию из *частей* неизвестных слов.  Например, \n",
    "\"unglobulate\" явно должно быть более вероятным чем \"zxfkogultae\".\n",
    "\n",
    "Для нашего следующего подхода мы используем идеи метода [Гуда - Тьюринга](https://en.wikipedia.org/wiki/Good–Turing_frequency_estimation). Он оценивает вероятности слов, не встретившихся в нашем Counter'е, на основании вероятностей слов, встретившихся единожды (Можно туда же подключить вероятности для встретившихся 2 раза и т.д.).\n",
    "\n",
    "<img src=\"http://upload.wikimedia.org/wikipedia/en/b/b4/I._J._Good.jpg\">\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a1/Alan_Turing_Aged_16.jpg/440px-Alan_Turing_Aged_16.jpg\" height=144 width=144>\n",
    "<br><i>Ирвинг Джон Гуд (1916 - 2009) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Алан Тьюринг (1812 - 1954)</i>\n",
    "\n",
    "Итак, сколько слов встретилось 1 раз в `COUNTS`?  (В `COUNTS1` ни одного такого слова нет.)  И какие длины у этих слов?  Давайте посмотрим:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(7, 1357),\n",
       " (8, 1356),\n",
       " (9, 1175),\n",
       " (6, 1113),\n",
       " (10, 938),\n",
       " (5, 747),\n",
       " (11, 627),\n",
       " (12, 398),\n",
       " (4, 368),\n",
       " (13, 215),\n",
       " (3, 159),\n",
       " (14, 112),\n",
       " (2, 51),\n",
       " (15, 37),\n",
       " (16, 10),\n",
       " (17, 7)]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singletons = (w for w in COUNTS if COUNTS[w] == 1)\n",
    "lengths = list(map(len, singletons))\n",
    "Counter(lengths).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1357 / sum(COUNTS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEACAYAAAC08h1NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f34ef50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist(lengths, bins=len(set(lengths)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Длины таких слов распределены похоже на нормальное распределение :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdist_good_turing_hack(counter, onecounter, base=1/26., prior=1e-8):\n",
    "    \"\"\"Вероятность слова при условии данных из счетчика.\n",
    "    Для неизвестных слов, смотрим на слова, встретившиеся единожды из onecounter, \n",
    "    вероятность выбираем, основываясь на длине.\n",
    "    Воспользуемся идеей метода Гуда-Тьюринга(полностью мы его здесь не реализуем).\n",
    "    prior -добавочный фактор, который сделает неизвестные слова менее вероятными.\n",
    "    base -то, насколько мы уменьшаем вероятность за длину слова больше максимального.\"\"\"\n",
    "    N = sum(list(counter.values()))\n",
    "    N2 = sum(list(onecounter.values()))\n",
    "    lengths = list(map(len, [w for w in onecounter if onecounter[w] == 1]))\n",
    "    ones = Counter(lengths)\n",
    "    longest = max(ones)\n",
    "    return (lambda word: \n",
    "            counter[word] / N if (word in counter) \n",
    "            else prior * (ones[len(word)] / N2 or \n",
    "                          ones[longest] / N2 * base ** (len(word)-longest)))\n",
    "#Переопределим P1w\n",
    "P1w = pdist_good_turing_hack(COUNTS1, COUNTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'test',\n",
       " 'of',\n",
       " 'segmentation',\n",
       " 'of',\n",
       " 'a',\n",
       " 'very',\n",
       " 'long',\n",
       " 'sequence',\n",
       " 'of',\n",
       " 'words']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment.cache.clear()\n",
    "segment('thisisatestofsegmentationofaverylongsequenceofwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(11) Задача: Что если слово находится очень далеко по edit_distance, но звучит точно так же?\n",
    "===\n",
    "\n",
    "Часто можно встретить ошибки в текстах, вызванные неграмотным написанием слов. Особенно часто это происходит в случае иностранных фамилий или транслитерированной терминологии. Обычно в таких случаях в пример приводят написание фамилии\n",
    "\n",
    "    Schwartzenegger\n",
    "    \n",
    "    в виде:\n",
    "    \n",
    "    Shwarzenegger, Shwortsinneger, schwartzineger ... und so weiter\n",
    "    \n",
    "Для такого случая можно использовать следующую методологию. Давайте привлечем лингвистов и составим правила, которые одинаково звучащим словам будут ставить в соответствие один и тот же код. Допустим, с помощью лингвистов мы такой алгоритм придумали. Тогда дальнейшие наши действия таковы:\n",
    "\n",
    "    1) Сделать словарь с вероятностями слов (как мы делали из мешка слов)\n",
    "\n",
    "    2) Сделать словарь соответствий код слова -> слово (с помощью того самого алгоритма от лингвистов). \n",
    "        Если есть в списке есть слова с одинаковым кодом, выбирать будем наиболее частое слово.\n",
    "\n",
    "    3) Сделаем аналогичный edit_distance алгоритм на множестве кодов слов\n",
    "\n",
    "    4) Найдя соответствующую замену для слова в виде его кода, восстановим слово с помощью словаря из пункта 2\n",
    "\n",
    "Алгоритм, про который мы поговорим, называется Double Metaphone. Примеры есть [тут](https://towardsdatascience.com/python-tutorial-fuzzy-name-matching-algorithms-7a6f43322cc5). Перейдем к делу, посмотрим, как это работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metaphone import doublemetaphone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм возвращает кортеж из двух возможных фонетических кодов слова. Правило такое:\n",
    "    # (Primary Key = Primary Key) = Идеальное совпадение\n",
    "    # (Secondary Key = Primary Key) = Совпадение\n",
    "    # (Primary Key = Secondary Key) = Совпадение\n",
    "    # (Alternate Key = Alternate Key) = Совпадение +-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идельное совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('KN0RKRS', 'KNTRKRS')\n",
      "('KN0RKRS', 'KNTRKRS')\n"
     ]
    }
   ],
   "source": [
    "print(doublemetaphone(\"Günther Graß\"))\n",
    "print(doublemetaphone(\"Günther Grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Совпадение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('XRTSNKR', 'XFRTSNKR')\n",
      "('XRTSNJR', 'XRTSNKR')\n"
     ]
    }
   ],
   "source": [
    "print(doublemetaphone(\"schwartzenegger\"))\n",
    "print(doublemetaphone(\"shwortsineger\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SRKS', '')\n",
      "('SRKS', '')\n"
     ]
    }
   ],
   "source": [
    "print(doublemetaphone(\"xerox\"))\n",
    "print(doublemetaphone(\"zeeerux\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/ru/d/d6/Логотип_НИУ_ВШЭ.jpg\" width = 200>\n",
    "\n",
    "\n",
    "# Полезные ссылки\n",
    "\n",
    "\n",
    "## [**Центр непрерывного образования ФКН ВШЭ**](https://cs.hse.ru/dpo/)\n",
    "\n",
    "\n",
    "* 18 августа 19:00 вебинар [«Определение цены для нового товара»](https://cs.hse.ru/dpo/announcements/383428707.html)\n",
    "* 26 августа 19:00 вебинар [«RFM-анализ: как выявить и удержать ключевых клиентов»](https://cs.hse.ru/dpo/announcements/383396631.html?__t=5516491&_r=66171596364525.81566&__r=OK)\n",
    "\n",
    "**Программы профессиональной переподготовки:**\n",
    "\n",
    "* [«Специалист по Data Science»](https://cs.hse.ru/dpo/datascientist) Старт 2 сентября\n",
    "\n",
    "* [«Аналитик данных»](https://cs.hse.ru/dpo/analyst) Старт 8 сентября\n",
    "\n",
    "### Соцсети\n",
    "\n",
    "**Facebook:** https://www.facebook.com/hsecs/ \n",
    "\n",
    "**VK:** https://vk.com/cshse  \n",
    "\n",
    "**Telegram:** https://t.me/fcs_hse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
